{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNU1C+nVDqsnQk1ucofYDjr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobstac/topic-modeling/blob/master/topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLRwsqcOlN5e"
      },
      "source": [
        "This notebook is based on the guide: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AnmZDivtsRq"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY2SXtWPuCIq"
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4TzAIHtsPE5",
        "outputId": "f5dcdea6-e4ca-4587-f14c-432c6eb2b582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# for lemmatization, converts to root word, e.g \"baking\" --> \"bake\"\n",
        "import spacy\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc0AG87g0Bpd"
      },
      "source": [
        "LDA creates a topic collection for each document. A topic is a collection of keywords. for instance if the topic is breakfast we could use keywords such as [juice, coffee, toast, egs, oatmeal, cornflakes]\n",
        "\n",
        "stopwords = words that are filtered out before processing of natural language data (text)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghWTwM-b0rPF",
        "outputId": "8d03f2dc-638e-4779-d6b0-4a684b9fc535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "print(stop_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'from', 'subject', 're', 'edu', 'use']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3ZwWl6F4_le"
      },
      "source": [
        "The newgroups data set contains 20 unique topics as printed below. The amount of post are around 11000. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxz1ufan4Umw",
        "outputId": "21984462-2ff4-484c-f31d-c279a2e7a5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "# Import Dataset\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
            " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
            " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
            " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
            " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
            " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...</td>\n",
              "      <td>16</td>\n",
              "      <td>talk.politics.guns</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
              "      <td>13</td>\n",
              "      <td>sci.med</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...</td>\n",
              "      <td>3</td>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
              "      <td>2</td>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ...              target_names\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...                 rec.autos\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...     comp.sys.mac.hardware\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...     comp.sys.mac.hardware\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...             comp.graphics\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...                 sci.space\n",
              "5  From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...  ...        talk.politics.guns\n",
              "6  From: bmdelane@quads.uchicago.edu (brian manni...  ...                   sci.med\n",
              "7  From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...  ...  comp.sys.ibm.pc.hardware\n",
              "8  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...  ...   comp.os.ms-windows.misc\n",
              "9  From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...  ...     comp.sys.mac.hardware\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9zBKXqlHCn"
      },
      "source": [
        "Removing emails and newline chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpzbWAW7leN_",
        "outputId": "cb263768-b0fc-441c-9c75-2f379c322712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub(r'\\S*@\\S*\\s?', '', d) for d in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub(r'\\s+', ' ', d) for d in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(r\"\\'\", \"\", d) for d in data]\n",
        "\n",
        "pprint(data[:2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
            " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
            " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
            " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
            " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
            " 'addition, the front bumper was separate from the rest of the body. This is '\n",
            " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
            " 'production, where this car is made, history, or whatever info you have on '\n",
            " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
            " 'your neighborhood Lerxst ---- ',\n",
            " 'From: (Guy Kuo) Subject: SI Clock Poll - Final Call Summary: Final call for '\n",
            " 'SI clock reports Keywords: SI,acceleration,clock,upgrade Article-I.D.: '\n",
            " 'shelley.1qvfo9INNc3s Organization: University of Washington Lines: 11 '\n",
            " 'NNTP-Posting-Host: carson.u.washington.edu A fair number of brave souls who '\n",
            " 'upgraded their SI clock oscillator have shared their experiences for this '\n",
            " 'poll. Please send a brief message detailing your experiences with the '\n",
            " 'procedure. Top speed attained, CPU rated speed, add on cards and adapters, '\n",
            " 'heat sinks, hour of usage per day, floppy disk functionality with 800 and '\n",
            " '1.4 m floppies are especially requested. I will be summarizing in the next '\n",
            " 'two days, so please add to the network knowledge base if you have done the '\n",
            " 'clock upgrade and havent answered this poll. Thanks. Guy Kuo ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqkuMOZ2lM2L"
      },
      "source": [
        "**We need to tokenize the text and do some cleaning**\n",
        "This is so the LDA can consume it. Gensim has something called simple_preprocess that can help us with this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7lBRPaAovVh",
        "outputId": "dfcb3850-eeb5-487e-c637-e191563be8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "  for sentence in sentences:\n",
        "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], ['from', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'of', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'floppies', 'are', 'especially', 'requested', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'havent', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvz3Tu5Z28tU"
      },
      "source": [
        "**Creating Bigram and Trigram Models**\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGmU6KsyoZvM",
        "outputId": "ab54dd7e-01e1-4499-ae21-a8461eff50b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p04GXWGt3j9w",
        "outputId": "5739a6e5-4a5d-49a8-89a8-9e9d57a123c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(trigram_mod[data_words[10]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution_usa', 'expires_sat', 'may', 'gmt', 'organization', 'computrac_inc', 'richardson_tx', 'keywords', 'ducati', 'gts', 'how', 'much', 'lines', 'have', 'line', 'on', 'ducati', 'gts', 'model', 'with', 'on', 'the', 'clock', 'runs', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'bit', 'of', 'oil', 'and', 'pops', 'out', 'of', 'st', 'with', 'hard', 'accel', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil_leak', 'they', 'sold', 'the', 'bike', 'to', 'the', 'and', 'only', 'owner', 'they', 'want', 'and', 'am', 'thinking', 'more', 'like', 'any', 'opinions', 'out', 'there', 'please', 'email', 'me', 'thanks', 'it', 'would', 'be', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', 'then', 'ill', 'get', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', 'tuba', 'irwin', 'honk', 'therefore', 'am', 'computrac', 'richardson_tx', 'dod']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_NLb4Qr7_uM"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDJ9yNCL8L68"
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg4i1vwF_QoK",
        "outputId": "8ceb954c-5758-4dc6-94cd-186130dd6a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(data_lemmatized[1:2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['poll', 'final', 'call', 'summary', 'final', 'call', 'clock', 'report', 'keyword', 'acceleration', 'line', 'host', 'fair', 'number', 'brave', 'soul', 'upgrade', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'hour', 'usage', 'day', 'functionality', 'floppy', 'especially', 'request', 'summarize', 'next', 'day', 'add', 'network', 'knowledge', 'base', 'do', 'clock', 'upgrade', 'answer', 'poll', 'thank']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NGoYEQyAdeb"
      },
      "source": [
        "We now need to create a input for the LDA topic model, namely:dictionary and corpus\n",
        "\n",
        "we see that (8, 2), means that word #8 (which is \"door\") appears twice in the first document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M4ChodvAufw",
        "outputId": "49257fd3-3e44-4d99-a91a-927b3288c401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "print(id2word[8])\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "door\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)], [(4, 2), (7, 2), (14, 1), (19, 1), (36, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 3), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1)], [(6, 3), (7, 1), (15, 2), (16, 1), (19, 2), (20, 2), (22, 1), (29, 2), (36, 1), (39, 1), (43, 1), (56, 1), (63, 1), (77, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 2), (92, 2), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 2), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 3), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 3), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 2), (139, 1), (140, 1), (141, 1), (142, 1), (143, 2), (144, 1), (145, 1), (146, 1), (147, 1), (148, 2), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 2), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 2), (173, 1), (174, 1), (175, 1)], [(14, 1), (16, 1), (19, 1), (20, 1), (29, 1), (37, 1), (65, 1), (110, 1), (113, 1), (174, 1), (176, 1), (177, 1), (178, 2), (179, 1), (180, 2), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 2)], [(16, 3), (29, 1), (31, 1), (37, 1), (39, 1), (105, 2), (130, 1), (134, 1), (148, 2), (149, 1), (151, 1), (174, 1), (177, 1), (197, 1), (198, 1), (199, 1), (200, 2), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 4), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 2), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 3), (232, 1)], [(22, 2), (36, 1), (40, 1), (65, 1), (80, 1), (81, 1), (87, 1), (88, 1), (112, 1), (115, 1), (139, 2), (169, 2), (174, 2), (177, 5), (188, 1), (197, 5), (202, 1), (220, 3), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 3), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 2), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 3), (273, 3), (274, 1), (275, 1), (276, 2), (277, 1), (278, 1), (279, 1), (280, 2), (281, 1), (282, 1), (283, 2), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 4), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 2), (312, 2), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 12)], [(6, 1), (15, 1), (19, 1), (21, 1), (36, 3), (88, 1), (103, 1), (120, 1), (139, 1), (155, 1), (174, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 2), (333, 1), (334, 1)], [(14, 1), (16, 1), (19, 1), (22, 1), (31, 1), (65, 1), (89, 5), (110, 1), (113, 2), (127, 1), (130, 1), (131, 1), (133, 2), (169, 3), (172, 1), (177, 2), (178, 6), (197, 1), (221, 1), (300, 2), (302, 1), (306, 1), (308, 1), (316, 1), (331, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 3), (350, 1), (351, 1), (352, 1), (353, 4), (354, 3), (355, 1), (356, 2), (357, 1), (358, 1), (359, 1), (360, 1), (361, 2), (362, 1), (363, 2), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (370, 3), (371, 1), (372, 1), (373, 1), (374, 1), (375, 1), (376, 2), (377, 1), (378, 2), (379, 1), (380, 5), (381, 1), (382, 1), (383, 1), (384, 19), (385, 1), (386, 1), (387, 1), (388, 1), (389, 1), (390, 2), (391, 1), (392, 1), (393, 1), (394, 1), (395, 1), (396, 1), (397, 1), (398, 3), (399, 2), (400, 1)], [(19, 1), (21, 1), (107, 1), (174, 1), (243, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 2), (406, 3), (407, 1), (408, 1), (409, 1), (410, 2)], [(19, 1), (22, 1), (40, 1), (88, 3), (103, 1), (107, 1), (116, 3), (133, 1), (155, 1), (169, 1), (174, 1), (177, 1), (184, 1), (197, 2), (211, 1), (216, 1), (250, 1), (268, 1), (300, 1), (324, 3), (330, 1), (343, 1), (378, 3), (382, 1), (386, 1), (406, 2), (411, 1), (412, 2), (413, 8), (414, 1), (415, 1), (416, 1), (417, 1), (418, 1), (419, 1), (420, 2), (421, 2), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 2), (431, 1), (432, 2), (433, 1), (434, 1), (435, 1), (436, 2), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 2), (443, 1), (444, 1), (445, 1), (446, 1), (447, 2), (448, 1), (449, 1), (450, 1), (451, 1), (452, 2), (453, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDtq9Ocd4-kg",
        "outputId": "ed5524f2-11ac-4c14-b579-43d2e91dfcf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "id2word[100]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'distribution_usa'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPB-pbYv3n0G",
        "outputId": "b3b437fb-6a12-43cd-b245-4803ca11a5ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        }
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[1:2]]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('call', 2),\n",
              "  ('day', 2),\n",
              "  ('host', 1),\n",
              "  ('line', 1),\n",
              "  ('thank', 1),\n",
              "  ('acceleration', 1),\n",
              "  ('add', 2),\n",
              "  ('answer', 1),\n",
              "  ('attain', 1),\n",
              "  ('base', 1),\n",
              "  ('brave', 1),\n",
              "  ('brief', 1),\n",
              "  ('card', 1),\n",
              "  ('clock', 3),\n",
              "  ('cpu', 1),\n",
              "  ('detailing', 1),\n",
              "  ('do', 1),\n",
              "  ('especially', 1),\n",
              "  ('experience', 2),\n",
              "  ('fair', 1),\n",
              "  ('final', 2),\n",
              "  ('floppy', 1),\n",
              "  ('functionality', 1),\n",
              "  ('hour', 1),\n",
              "  ('keyword', 1),\n",
              "  ('knowledge', 1),\n",
              "  ('message', 1),\n",
              "  ('network', 1),\n",
              "  ('next', 1),\n",
              "  ('number', 1),\n",
              "  ('oscillator', 1),\n",
              "  ('poll', 3),\n",
              "  ('procedure', 1),\n",
              "  ('rate', 1),\n",
              "  ('report', 1),\n",
              "  ('request', 1),\n",
              "  ('send', 1),\n",
              "  ('share', 1),\n",
              "  ('soul', 1),\n",
              "  ('speed', 2),\n",
              "  ('summarize', 1),\n",
              "  ('summary', 1),\n",
              "  ('top', 1),\n",
              "  ('upgrade', 2),\n",
              "  ('usage', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev9doYcv4uGV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1atkQg9T5Rcq"
      },
      "source": [
        "Now we need to **build our model**\n",
        "the number of topics needs to be chosen\n",
        "chunk size is like batch size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqlEzZC5rOK"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}